{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Leaderboards\n",
    "- [MPI Sintel](http://sintel.is.tue.mpg.de/results)\n",
    "- [MiddleBury](http://vision.middlebury.edu/flow/eval/results/results-e1.php)\n",
    "- [KITTI](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brightness Constancy Model\n",
    "$$I(x+u, y+v, t+1) = I(x, y, t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 挑战\n",
    "## Large Displacement\n",
    "## Occlusion\n",
    "## 实时性要求\n",
    "Sparse\n",
    "BriefMatch\n",
    "\n",
    "- NNF的速度为\n",
    "- EpicFlow的速度大概在16s/帧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用意义\n",
    "通过sparse光流预测出dense光流, 来提高更高级任务的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "## 其他\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.562&rep=rep1&type=pdf\n",
    "## Single-view Feature Detectors\n",
    "- ICCV2013 | Piotr Dollár *et al.* **\"Structured Forests for Fast Edge Detection\"** [pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2013/12/DollarICCV13edges.pdf)\n",
    "    > SEDv1, Edge Detector, EpicFlow中引用的是这个\n",
    "- PAMI2015 | Piotr Dollár *et al.* **\"Fast Edge Detection Using Structured Forests\"** [arXiv](https://arxiv.org/abs/1406.5549)\n",
    "    > SEDv2, Edge Detector, OpenCV的实现是这个\n",
    "- CVPR2016 | Yin Li *et al.* **\"Unsupervised Learning of Edges\"** [arXiv](https://arxiv.org/abs/1511.04166) [code](https://github.com/happyharrycn/unsupervised_edges)\n",
    "\n",
    "## Multi-views Sparse Feature Detectors\n",
    "- PAMI2012 | L. Xu *et al.* **\"Motion detail preserving optical flow estimation\"**\n",
    "\n",
    "- ICCV2013 | Philippe Weinzaepfel *et al.* **\"DeepFlow: Large displacement optical flow with deep matching\"** [pdf](https://hal.inria.fr/hal-00873592/document)\n",
    "- WACV2015 | Radu Timofte *et al.* **\"SparseFlow: Sparse Matching for Small to Large Displacement Optical Flow\"** [pdf](https://homes.esat.kuleuven.be/~konijn/publications/2015/Timofte-WACV-2015.pdf)\n",
    "- IJCV2016 | Jerome Revaud *et al.* **\"DeepMatching: Hierarchical Deformable Dense Matching\"** [arXiv](https://arxiv.org/abs/1506.07656)\n",
    "\n",
    "## Multi-views Dense Feature Detectors\n",
    "- CVPR2015 | Jiaolong Yang *et al.* **\"Dense, Accurate Optical Flow Estimation with Piecewise Parametric Model\"** [pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yang_Dense_Accurate_Optical_2015_CVPR_paper.pdf)\n",
    "\n",
    "\n",
    "## Coarse-to-Fine\n",
    "\n",
    "\n",
    "\n",
    "- CVPR2013 | Zhuoyuan Chen *et al.* **\"Large Displacement Optical Flow from Nearest Neighbor Fields\"** [pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Large_Displacement_Optical_2013_CVPR_paper.pdf)\n",
    "    > NNF\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICCV2015 | Christian Bailer *et al.* **\"Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation\"** [pdf](https://www.dfki.de/web/research/publications/renameFileForDownload?filename=FlowFields.pdf&file_id=uploads_2564)\n",
    "- CVPR2017 | Christian Bailer *et al.* **\"CNN-based Patch Matching for Optical Flow with Thresholded Hinge Embedding Loss\"** [arXiv](https://arxiv.org/abs/1607.08064)\n",
    "    > FlowFieldsCNN\n",
    "\n",
    "- T-PAMI | Christian Bailer *et al.* **\"Optical Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation\"** [arXiv](https://arxiv.org/abs/1703.02563)\n",
    "    > FlowFields+\n",
    "- ICIP2018 | WTF\n",
    "    > FlowFields++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CVPR2015 | Jiaolong Yang *et al.* **\"Dense, Accurate Optical Flow Estimation with Piecewise Parametric Model\"** [pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yang_Dense_Accurate_Optical_2015_CVPR_paper.pdf)\n",
    "    > PH-Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICCV2017 | Anne S. Wannenwetsch *et al.* **\"ProbFlow: Joint Optical Flow and Uncertainty Estimation\"** [arXiv](https://arxiv.org/abs/1708.06509)\n",
    "    > ProbFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ACCV2017 | Fatma Güney *et al.* **\"Deep Discrete Flow\"** [pdf](http://www.cvlibs.net/publications/Guney2016ACCV.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督方法 & Proxy\n",
    "\n",
    "- Unsupervised convolutional neural networks for motion estimation\n",
    "- ECCV2016 | Jason J. Yu *et al.* **\"Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness\"** [arXiv](https://arxiv.org/abs/1608.05842) []()\n",
    "    > UnsupFlownet\n",
    "\n",
    "> Unsupervised techniques such as ours are limited by the loss function. That is, it should be noted that the amount of information that can be gained from the available video data is limited by how faithfully the problem is modeled by the loss. -- ***UnFlow***\n",
    "\n",
    "\n",
    "- AAAI 2017 | Zhe Ren *et al.* **\"Unsupervised Deep Learning for Optical Flow Estimation\"** [pdf](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14388/13940) [code](https://github.com/sunshinezhe/Dense-Spatial-Transform-Flow)\n",
    "    > DSTFlow\n",
    "\n",
    "- CVPR2017 | Yi Zhu *et al.* **\"Guided Optical Flow Learning\"** [arXiv](https://arxiv.org/abs/1702.02295) [code](https://github.com/bryanyzhu/GuidedNet)\n",
    "    > GuidedNet, 先用传统方法生成GroundTruth指导CNN学习, 然后用Image Reconstruction Loss做无监督学习的思想.\n",
    "    论文用***FlowFields***做传统光流预测(选的是当时最好的), 用***FlowNet Simple***做CNN部分, 用EPE loss.\n",
    "- AAAI 2018 | Simon Meister *et al.* **\"UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss\"** [arXiv](https://arxiv.org/abs/1711.07837) [code](https://github.com/simonmeister/UnFlow)\n",
    "    > UnFlow, 进步性体现在loss设计上,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CVPR2017 | **\"S2F: Slow-To-Fast Interpolator Flow\"** [pdf](http://www.vision.cs.ucla.edu/papers/yangS17.pdf)\n",
    "    > S2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点: \n",
    "> Another limitation of our method is that we need to perform a parameter search for the weighting between the loss\n",
    "terms (e.g., the influence of the smoothness loss) for best results, which increases the total computation time for training a model on a new domain for the first time. This limitation is shared by previous works employing an unsupervised proxy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Refining\n",
    "- a\n",
    "    > median filtering\n",
    "- a\n",
    "    > weighted median filtering\n",
    "- a\n",
    "    > bilateral filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CVPR2010 | Deqing Sun *et al.* **\"Secrets of Optical Flow Estimation and Their Principles\"** [pdf](http://www-pequan.lip6.fr/~bereziat/cours/master/vision/papers/sun10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- CVPR2018 | Lijie Fan *et al.* **\"End-to-End Learning of Motion Representation for Video Understanding\"** [arXiv](https://arxiv.org/abs/1804.00413) [code](https://github.com/LijieFan/tvnet)\n",
    "    > TVNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigid & Non-rigid\n",
    "- CVPR2017 | *et al.* **\"Optical Flow in Mostly Rigid Scenes\"** [arXiv](https://arxiv.org/abs/1705.01352) [code](https://github.com/jswulff/mrflow)\n",
    "    > MR-Flow, 引入了Plane + Parallax方法, 在对待rigid scene和non-rigid scene的处理上, 这一篇通过segmentation将二者区分开, 对于纯rigid scene, 搜索复杂度从2D降到了1D, 进而引入了双目视觉中常用的P+P方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Check\n",
    "![](imgs/consistency_check.png)\n",
    "\n",
    "根据`bw_flow`(`tgt` to `src`)对`fw_flow`(`src` to `tgt`)作warp变换, 得到`bw_flow2`然后计算像素差\n",
    "\n",
    "- 如果forward flow和backward flow匹配, 那么一个点先根据forward flow移动, 再根据backward flow移动, 应该移回原位. 体现了consistency\n",
    "- 如果forward flow和backward flow不匹配, \n",
    "- 将forward flow和backward flow对齐:\n",
    "\n",
    "参考***CPM***:\n",
    "```cpp\n",
    "void CPM::CrossCheck(IntImage& seeds, FImage& seedsFlow, FImage& seedsFlow2, IntImage& kLabel2, int* valid, float th)\n",
    "{\n",
    "\tint w = kLabel2.width();\n",
    "\tint h = kLabel2.height();\n",
    "\tint numV = seeds.height();\n",
    "\tfor (int i = 0; i < numV; i++){\n",
    "\t\tvalid[i] = 1;\n",
    "\t}\n",
    "\n",
    "\t// cross check (1st step)\n",
    "\tint b = _borderWidth;\n",
    "\tfor (int i = 0; i < numV; i++){\n",
    "\t\tfloat u = seedsFlow[2 * i];\n",
    "\t\tfloat v = seedsFlow[2 * i + 1];\n",
    "\t\tint x = seeds[2 * i];\n",
    "\t\tint y = seeds[2 * i + 1];\n",
    "\t\tint x2 = x + u;\n",
    "\t\tint y2 = y + v;\n",
    "\t\tif (x < b || x >= w - b || y < b || y >= h - b\n",
    "\t\t\t|| x2 < b || x2 >= w - b || y2 < b || y2 >= h - b\n",
    "\t\t\t|| sqrt(u*u + v*v)>_maxDisplacement){\n",
    "\t\t\tvalid[i] = 0;\n",
    "\t\t\tcontinue;\n",
    "\t\t}\n",
    "\n",
    "\t\tint idx2 = kLabel2[y2*w + x2];\n",
    "\t\tfloat u2 = seedsFlow2[2 * idx2];\n",
    "\t\tfloat v2 = seedsFlow2[2 * idx2 + 1];\n",
    "\t\tfloat diff = sqrt((u + u2)*(u + u2) + (v + v2)*(v + v2));\n",
    "\t\tif (diff > th){\n",
    "\t\t\tvalid[i] = 0;\n",
    "\t\t}\n",
    "\n",
    "\t\n",
    "```\n",
    "\n",
    "\n",
    "python中利用矩阵运算来优化的话, 步骤如下\n",
    "```\n",
    "```\n",
    "\n",
    "\n",
    "- 假如forward flow: `src`上的(x,y)->`tgt`的(u,v)是右移40像素, 那么反过来说, forward flow也能说明`tgt`的(u,v)->`src`的是左移40像素. 公式上的表现就是把forward flow根据backward flow进行warp, (x,y)\n",
    "- 此时如果backward flow: `tgt`->`src`是左移40像素, 那么符合\"consistency\". 反之, 说明两张图的预测不匹配. 很可能出现了occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-92db8288c62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfwd_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/frame1.flo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_flow(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic = float(np.fromfile(f, np.float32, count = 1)[0])\n",
    "        if magic == 202021.25:\n",
    "            w = np.fromfile(f, np.int32, count = 1)[0]\n",
    "            h = np.fromfile(f, np.int32, count = 1)[0]\n",
    "            data = np.fromfile(f, np.float32, count = h*w*2)\n",
    "            data.resize((h, w, 2))\n",
    "            return data\n",
    "        return None\n",
    "    \n",
    "fwd_flow = np.ones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchMatch\n",
    "- SIGGRAPH2009 | Connelly Barnes *et al.* **\"PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing\"** [pdf](http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch.pdf)\n",
    "    > PatchMatch\n",
    "    \n",
    "- | Connelly Barnes *et al.* **\"The Generalized PatchMatch Correspondence Algorithm\"** [pdf](http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/generalized_pm.pdf)\n",
    "    > Generalized PatchMatch\n",
    "    \n",
    "- CVPR2016 | David (Dedi) Gadot *et al.* **\"PatchBatch: a Batch Augmented Loss for Optical Flow\"** [arXiv](https://arxiv.org/abs/1512.01815)\n",
    "    > 用Siamese CNN分别提取两张图片的descriptor, 然后用L2. 简单却卓有成效的原因在于设计精巧的loss\n",
    "\n",
    "- CVPR2016 | Yinlin Hu *et al.* **\"Efficient Coarse-to-Fine PatchMatch for Large Displacement Optical Flow\"** [pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hu_Efficient_Coarse-To-Fine_PatchMatch_CVPR_2016_paper.pdf) [code](https://github.com/YinlinHu/CPM)\n",
    "    > CPM, 一种dense的方法, 最后经过outlier handling变成sparse的, CPM + EpicFlow的结果优于DeepMatching + EpicFlow\n",
    "\n",
    "- T-CSVT | Yinlin Hu *et al.* **\"Coarse-to-fine PatchMatch for dense correspondence\"** [pdf](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959195) [code](https://github.com/YinlinHu/CPM)\n",
    "    > CPM的期刊版本, 性能似乎更好了\n",
    "\n",
    "- CVPR2017 | Tal Schuster *et al.* **\"Optical Flow Requires Multiple Strategies (but only one network)\"**[arXiv](https://arxiv.org/abs/1611.05607)\n",
    "    > 同一班人马, 改进了PatchBatch中的loss\n",
    "    \n",
    "-  SCIA2017 | Gabriel Eilertsen *et al.* **\"BriefMatch: Dense binary feature matching for real-time optical flow estimation\"** [pdf](http://vcl.itn.liu.se/publications/2017/EFU17/scia2017.pdf) [code](https://github.com/gabrieleilertsen/briefmatch)\n",
    "    > 用BRIEF描述子和PatchMatch的scheme\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowNet\n",
    "这一体系的特点是用CNNs来直接匹配Flow\n",
    "- ICCV2015 | Philipp Fischer *et al.* **\"FlowNet: Learning Optical Flow with Convolutional Networks\"** [arXiv](https://arxiv.org/abs/1504.06852) [code](https://github.com/ClementPinard/FlowNetPytorch)\n",
    "    > FlowNetSimple中把image pair堆叠起来feed给CNN, FlowNetCorr中提出Correlation Layer, 把之前的搜索+匹配过程融入CNN中. 养活了后续一大波论文. FlowNetSimple的ch_in = 6\n",
    "    \n",
    "- CVPR2017 | Eddy Ilg *et al.* **\"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\"** [arXiv](https://arxiv.org/abs/1612.01925) [code](https://github.com/NVIDIA/flownet2-pytorch)\n",
    "    > 改进, FlowNetSimple的ch_in = 12, 如图可见feed了很多乱七八糟的给CNN\n",
    "    ![](imgs/flownet2_overview.png)\n",
    "    \n",
    "- ICCV2017 | Junhwa Hur *et al.* **\"MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation\"** [arXiv](https://arxiv.org/abs/1708.05355) [code](https://bitbucket.org/visinf/projects-2017-mirrorflow)\n",
    "    > MirrorFlow, 以往的网络(暗示FlowNet2.0)都是预测出单向的flow, 通过过两遍CNN得出forward和backward的flow, 然后做consistency check. 其中的问题在于\n",
    "\n",
    "- CVPR2017 | Anurag Ranjan *et al.* **\"Optical Flow Estimation using a Spatial Pyramid Network\"** [arXiv](https://arxiv.org/abs/1611.00850) [code](https://github.com/anuragranj/spynet)\n",
    "    > SPyNet, 非常compact的网络, 体现出了CNN的美感, 然而性能上并不能满意. 类似FlowNet2.0, 每一层给CNN喂target, ref_warped和flow. 用Image Pyramids的方式帮助网络走一个coarse-to-fine的scheme, 也解决了large displacement的问题\n",
    "    \n",
    "- CVPR2017 | Yi Zhu *et al.* **\"Guided Optical Flow Learning\"** [arXiv](https://arxiv.org/abs/1702.02295) [code](https://github.com/bryanyzhu/GuidedNet)\n",
    "    > GuidedNet, 解决缺少synthetic data的问题. 先用传统方法生成GroundTruth指导CNN学习, 然后用Image Reconstruction Loss做无监督学习的思想.\n",
    "    论文用FlowFields做传统光流预测(选的是当时最好的), 用FlowNet Simple做CNN部分, 用EPE loss.\n",
    "    \n",
    "- CVPR2018 | Deqing Sun *et al.* **\"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\"** [arXiv](https://arxiv.org/abs/1709.02371)\n",
    "    > PWC-Net: 改进Image Pyramids为Learnable Feature Pyramids, 改进feed image pair为feed cost volume, 加入很多principles. 对principle的一段讨论: 提了一些未来工作的建议: 1. 加入scene flow 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## Photometric Losses\n",
    "- UnFlow:\n",
    "    > As the brightness constancy is not invariant to illumination changes common in realistic situations (Vogel, Schindler, and Roth 2013), we instead use the ternary census transform (Zabih and Woodfill 1994; Stein 2004). The census transform can compensate for additive and multiplicative illumination changes as well as changes to gamma (Hafner, Demetz, and Weickert 2013), thus providing us with a more reliable constancy assumption for realistic imagery.\n",
    "\n",
    "## Smoothness Losses\n",
    "- UnsupFlownet:\n",
    "    $$l_{smoothness}(u, v) = \\Sigma^H_j\\Sigma^W_i[\\rho_S(u_{i,j} - u_{i+1,j}) + \\rho_S(u_{i,j} - u_{i,j+1}) + \\rho_S(v_{i,j} - v_{i+1,j}) + \\rho_S(v_{i,j} - v_{i,j+1})]$$\n",
    "    \n",
    "- InterpoNet:\n",
    "    $$L_{ld} = \\frac{1}{n}\\Sigma_{i,j}|EPE(\\hat{Y}_{i,j}, \\hat{Y}_{i-1,j}) - EPE(Y_{i,j}, Y_{i-1,j})| + |EPE(\\hat{Y}_{i,j}, \\hat{Y}_{i,j-1}) - EPE(Y_{i,j}, Y_{i,j-1})|$$\n",
    "    > To imitate the lateral dependency between neighbors in the network, we define a new lateral dependency loss. This loss pushes the distance between neighboring pixels to be similar to the distance in the ground truth flow.\n",
    "    \n",
    "- UnFlow:\n",
    "    $$E_S(w^f, w^b) = \\Sigma_{x\\in P} \\Sigma_{(s,r)\\in N(x)}\\rho(w^f(s) - 2w^f(x) + w^f(r)) + \\rho(w^b(s) - 2w^b(x) + w^b(r))$$\n",
    "    > ..., we use a second-order smoothness constraint (Trobin et al. 2008; Zhang et al. 2014) on the flow field to encourage collinearity of neighboring flows and thus achieve more effective regularization. -- ***UnFlow***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plane + Parallax\n",
    "- CVPR2017 | *et al.* **\"Optical Flow in Mostly Rigid Scenes\"** [arXiv](https://arxiv.org/abs/1705.01352) [code](https://github.com/jswulff/mrflow)\n",
    "    > MR-Flow, 引入了Plane + Parallax方法, 在对待rigid scene和non-rigid scene的处理上, 这一篇通过segmentation将二者区分开, 对于纯rigid scene, 搜索复杂度从2D降到了1D, 进而引入了双目视觉中常用的P+P方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-to-Dense Interpolation\n",
    "输入可能是极稀疏的映射(传统方法), 也可能是semi-dense(很多方法把occlusions当做outliers滤掉了)\n",
    "\n",
    "- CVPR 2015 | Jerome Revaud *et al.* **\"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\"** [arXiv](https://arxiv.org/abs/1501.02565)\n",
    "    > 引入两种插值方法, 通过相邻帧的pair + SED先验的contours和DeepMatching得到的sparse flow来生成dense flow.\n",
    "    使用了启发式的算法, 速度不尽人意, 非学习方法\n",
    "- ECCV2016 | Yu Li *et al.* **\"Fast Guided Global Interpolation for Depth and Motion\"** [pdf](http://publish.illinois.edu/visual-modeling-and-analytics/files/2016/08/FGI.pdf) [code](https://github.com/yu-li/fgi)\n",
    "    > FGI\n",
    "- CVPR2017 | Shay Zweig *et al.* **\"InterpoNet, A brain inspired neural network for optical flow dense interpolation\"** [arXiv](https://arxiv.org/pdf/1611.09803.pdf) [code](https://github.com/shayzweig/InterpoNet)\n",
    "    > InterpoNet, 自称首个sparse-to-dense interpolation的学习方法.    \n",
    "    以DeepMatching/DiscreteFlow/FlowFields/CPM作前端, InterpoNet的效果均优于EpicFlow. 可作为新的base Interpolator     \n",
    "    提出了新的lateral dependency loss, 把相邻像素之间的关系加入到了学习过程中    \n",
    "    同样用SED作edge detector\n",
    "    \n",
    "- CVPR2017 | Jia Xu *et al.* **\"Accurate Optical Flow via Direct Cost Volume Processing\"** [arXiv](https://arxiv.org/abs/1704.07325) [code](https://github.com/IntelVCL/dcflow)\n",
    "    > DCFlow, 指出EpicFlow在matches过少时性能不好, 并提出了新的interpolation. 大部分光流可以用Planar Homographies来表示,\n",
    "    \n",
    "    > EpicFlow uses locally-weighted affine models to synthesize a dense flow field from semi-dense matches. We found that this scheme yields accurate results in areas where input matches are fairly dense, but is less reliable when large occluded regions must be filled. --***DCFlow***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Volume\n",
    "cost volume, 感觉上就是解决不能很好地从raw image提取光流特征\n",
    "- CVPR2016 | Qifeng Chen *et al.* **\"Full Flow: Optical Flow Estimation By Global Optimization over Regular Grids\"** [arXiv](https://arxiv.org/abs/1604.03513) [website](http://cqf.io/fullflow/)\n",
    "    > FullFlow, 没有基于CNNs, 更方便理解cost volume是如何工作的(然后CNNs会找到更优的拟合函数)\n",
    "    ![](imgs/fullflow_grid.png)\n",
    "- CVPR2017 | Jia Xu *et al.* **\"Accurate Optical Flow via Direct Cost Volume Processing\"** [arXiv](https://arxiv.org/abs/1704.07325) [code](https://github.com/IntelVCL/dcflow)\n",
    "    > DCFlow, 根据两张downsample的image求cost volume\n",
    "\n",
    "- CVPR2018 | Deqing Sun *et al.* **\"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\"** [arXiv](https://arxiv.org/abs/1709.02371)\n",
    "    > PWC-Net: 改进1: 由image计算变为feature计算; 改进2: 由full cost volume变为spatial cost volume计算, 降低了计算量, 因为金字塔高层的\n",
    "    \n",
    "    range为4的cost volume计算\n",
    "    \n",
    "| Range of Cost Volume | Resolution | Pixels |\n",
    "| -------------------- | ---------- | ------ |\n",
    "| 2                    |  1024×436  |   200  |\n",
    "| 4                    |  1024×436  |   2    |\n",
    "    \n",
    "    改进Image Pyramids为Learnable Feature Pyramids, 改进feed image pair为feed cost volume, 加入很多principles. 对principle的一段讨论: 提了一些未来工作的建议: 1. 加入scene flow 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 从问题的角度看……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Images难以提取关联性\n",
    "> First, it is well-known that raw images do not provide good features to establish correspondence, particularly in the presence of shadows and lighting changes. --***PWC-Net***\n",
    "\n",
    "- 加入了contour\n",
    "- ***DCFlow***和***PWC-Net***中提出了Cost Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Displacement\n",
    "- Coarse-to-Fine scheme: ***FlowNetSimple***中很放心地把stack后的image pair送给网络, 从CNN的计算原理可以得知, 如果小物体大位移的情况严重到第一帧物体在receptive field中, 第二帧物体不在, 那么光流预测就无从下手了. ***SPyNet***引入Image Pyramids, 一开始给模型feed低分辨率但空间跨度大的图像, 并把每个level的光流预测结果upscale后传到下一个level(塔尖到塔底), 一定程度解决了上述问题, 比如模型的某一level可能学到: 第一帧有一个物体, 第二帧物体不见了, 上层的光流中显示这个物体朝右运动, 那么我的预测结果也强化这个结论. small and fast moving objects在coarse levels会消失\n",
    "\n",
    "> - One conundrum for the coarse-to-fine, variational approach is small and fast moving objects that disappear at coarse levels. --***PWC-Net***\n",
    "- A major drawback of coarse-to-fine schemes is error-propagation, i.e., errors at coarser levels, where different motion layers can overlap, can propagate across scales. --***EpicFlow***\n",
    "\n",
    "- Feature Matching: 基本不会遇到这个问题, 调查各个matching方法有没有加入位移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion\n",
    "以往的occlusion检测都是死规则(Forward-Backward Consistency Check/Loss), 这个方法的过程是:\n",
    "1. 预测forward flow和backward flow(跑两遍inference)\n",
    "2. \n",
    "\n",
    "这\n",
    "***MirrorFlow***中\n",
    "\n",
    "\n",
    "在Sparse-to-Dense Interpolation的贡献中, 也有对occlusion的改进方案. ***DCFlow***批判了***EpicFlow***在极少matches时(对应前端出现大量occlusion, filtering后)性能不好, 提出新的interpolation方案来改进大量occlusion时的效果.\n",
    "\n",
    "> EpicFlow uses locally-weighted affine models to synthesize a dense flow field from semi-dense matches. We found that this scheme yields accurate results in areas where input matches are fairly dense, but is less reliable when large occluded regions must be filled. --***DCFlow***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "- 基于Matching的方法(PatchMatch等网络)普遍慢于直接法(FlowNet等网络), 这是因为当光流预测被看做搜索-匹配任务时, 它的搜索空间是2D的整张图像. (e.g. ***EPICFlow***中DeepMatching占了大量时间), PatchMatch中提出了一些解决办法.\n",
    "\n",
    "- ***MR-Flow***把图像分割成rigid和non-rigid两部分, rigid部分的flow就纯粹是因ego-motion和depth而引起的disparity导致的, 搜索空间从2D降到了1D. 采用Plane + Parallax的方法\n",
    "- ***DCFlow***和***PWC-Net***用了Cost Volume来作为表示.\n",
    "    - ***DCFlow***: CNN提特征, 以Cost Volume来表示, 然后用后处理策略,\n",
    "    - ***PWC-Net***: \n",
    "- Energy Minimization\n",
    "\n",
    "> - However, optimizing a complex energy function is usually computationally expensive for real-time applications. --***PWC-Net***\n",
    "- Compared with energy minimization, the warping, cost volume, and CNN layers are computationally light. --***PWC-Net***\n",
    "\n",
    "- 对于直接法, 进一步提速的方案是减小模型size和flops, 这里有一些网络参数量的整理\n",
    "\n",
    "| Network | Size | FPS |\n",
    "|---|---|---|\n",
    "| ***FlowNet*** |asd|sad|\n",
    "| ***SPyNet*** | | |\n",
    "| ***PWC-Net*** | asd | qw |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth from Single View\n",
    "\n",
    "> We also experimented with using multiple views as input to the depth network, but did not find this to improve the results. This is in line with the observations in [47], where optical flow constraints need to be enforced to utilize multiple views effectively. --***Unsupervised***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Minimum during Training\n",
    "> However, we observe in our experiments that a deeper optical flow estimator might get stuck in a poor local minimum, which can be detected early by checking the validation errors after a few thousand iterations and fixed by a complete new run. --***PWC-Net***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Transform\n",
    "> It is computationally cheap, illumination resistant and to some extend edge aware. -- ***FlowFields***\n",
    "\n",
    "- ***FlowFields***作为一种基于传统feature的matching方法, 使用census transform\n",
    "- ***UnFlow***中将census transform以loss的形式引入到学习过程, 提出了Bidirectional Census Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 能做的工作\n",
    "## 进一步提高FPS\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
